# -*- coding: utf-8 -*-
"""300_1e-5_emotional-classification-hi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EW3ha3AwcSV6lRDfjDF5ynlx_My2zDnc
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install elmoformanylangs

import torch
import os
import numpy as np
import pandas as pd

import torch.nn as nn
import torch.optim

from torch.nn import functional as F

from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils

from math import ceil
from elmoformanylangs import Embedder

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

classes = ['angry', 'neutral', 'sad', 'happy']
texts = []
labels = []
for idx, label in enumerate(classes):
    PATH = 'drive/My Drive/emotional-classification-hindi/emotions/'+str(label)
    for files in os.walk(PATH):
        for item in files[2]:
            filename = PATH + str('/') + item
            texts.append(open(filename, 'r', encoding='utf-8').read())
            labels.append(idx)

print(texts)
print(labels)

e = Embedder('drive/My Drive/emotional-classification-hindi/155')

sent = []
for item in texts:
    sent.append(item.split(' '))
    
sents = []
for item in sent:
    sents.append(item[:len(item)-1])

print(sents)

vec = e.sents2elmo(sents)

maxim = 0
for item in vec:
    if len(item) >= maxim:
        maxim = len(item)

print(maxim)

minim = maxim
for item in vec:
    if len(item) <= minim:
        minim = (len(item))
print(minim)

padded = []
for i in range (len(vec)):
    siz = maxim - int(vec[i].shape[0])
    zer = np.zeros((siz, 1024), dtype=float)
    padded.append((np.append(vec[i], zer, axis = 0)))

col_names = ['labels','vector']

df_train = pd.DataFrame({'vector':padded, 'labels':labels},
                        columns=col_names)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df_train, labels, test_size=0.1, shuffle=True)
X_tr = X_train['vector']
X_ts = X_test['vector']
X_tr = list(X_tr)
X_ts = list(X_ts)

def trainData(arr, y, batch = 8, istrain = True):
    class txtData(torch.utils.data.Dataset):
        def __init__(self, arr, y, istrain, transform, device='cuda'):
            self.arr = arr
            self.labels = y
            self.istrain = istrain
            self.transform = transform

        def __len__(self):
            return len(self.labels)

        def __getitem__(self, idx):
            data = self.arr[idx]
            label = self.labels[idx]


            if self.istrain:
                return data, label

    data_transform = transforms.Compose([
                transforms.ToTensor()])
    dataset = txtData(arr, y, istrain=istrain, transform=data_transform)
    dataloader = DataLoader(dataset, batch_size = batch, shuffle=True, num_workers=2)

    return dataloader

train_loader = trainData(X_tr, y_train)
test_loader = trainData(X_ts, y_test, batch = 52)

class mainNet(nn.Module):
    def __init__(self):
        super(mainNet, self).__init__()

        self.lstm = nn.LSTM(input_size=1024, hidden_size=512, num_layers=1, bidirectional=False)
        self.fc = nn.Sequential(
                nn.Linear(512, 4),
                nn.ReLU(inplace=False)
                )
        self.dp1 = nn.Dropout(0.5)
        self.softmax = nn.Softmax(dim = 1)

    def forward(self, main):
        main[main != main] = 0.5
        h0 = torch.zeros(1, main.size(0), 512)
        c0 = torch.zeros(1, main.size(0), 512)
        main = main.permute(1, 0, 2)
        output, (final_hidden_state, final_cell_state) = self.lstm(main, (h0, c0))
        output[output != output] = 0.5
        output = output.permute(1,0, 2)
        hidden = final_hidden_state.squeeze(0)
        attn_weights = torch.bmm(output, hidden.unsqueeze(2)).squeeze(2)
        soft_attn_weights = F.softmax(attn_weights, 1)
        x = new_hidden_state = torch.bmm(output.transpose(1,2), soft_attn_weights.unsqueeze(2)).squeeze(2)

        x = x.to(device)
        out = self.fc(x)
        out = self.dp1(out)
        fin_out = self.softmax(out)

        return fin_out

device = torch.device("cpu")
model = mainNet().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
loss_function = torch.nn.CrossEntropyLoss()

num_epochs = 300
losses = []

trn_accuracy = []
val_accuracy = []

for epoch in range(num_epochs):
    print("epoch: {}/{}".format(epoch, num_epochs))
    train_loss = []
    valid_loss = []
    ep_acc = []
    model.train()

    main_iter = iter(train_loader)
    i = 0
    while (i < len(train_loader)):
        accuracy = 0
        mainiter = main_iter.next()

        main, label = mainiter
        main = main.to(device)
        label = label.to(device)
        
        optimizer.zero_grad()

        main = torch.Tensor.float(main)

        output = model(main)

        _, idx = torch.max(output, dim=1)
        acc = (idx == label)
        accuracy += acc.sum().item() / acc.size(0)

        loss = loss_function(output, label)

        loss.backward()
        optimizer.step()
        i+=1
        ep_acc.append(accuracy)
        train_loss.append(loss.item())

    print("Training")
    trn_accuracy.append((sum(ep_acc)/len(ep_acc)))
    print(sum(ep_acc)/len(ep_acc))


    model.eval()
    ep_acc = []

    main_iter_v = iter(test_loader)
    i = 0
    while (i < len(test_loader)):
        accuracy = 0
        mainiter = main_iter_v.next()
  
        main, label = mainiter
        main = main.to(device)
        label = label.to(device)

        optimizer.zero_grad()

        main = torch.Tensor.float(main)
        output = model(main)

        _, idx = torch.max(output, dim=1)
        acc = (idx == label)
        accuracy += acc.sum().item() / acc.size(0)


        loss = loss_function(output, label)
        i+=1
        ep_acc.append(accuracy)
        train_loss.append(loss.item())
        valid_loss.append(loss.item())

    losses.append([train_loss, valid_loss])
    print("Validation")
    vl_accr = sum(ep_acc)/len(ep_acc)
    val_accuracy.append(vl_accr)
    print(vl_accr)

    best_accuracy = 0
    if vl_accr >= best_accuracy:
        torch.save(model, 'model-'+str(epoch)+str('_1e-5')+'.pth')
        best_accuracy = (sum(ep_acc)/len(ep_acc))